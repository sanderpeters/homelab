Name:             clamav-7f69848b68-6k7kz
Namespace:        homelab
Priority:         0
Service Account:  clamav
Node:             homelab/192.168.2.100
Start Time:       Sun, 21 Dec 2025 13:49:54 +0100
Labels:           app.kubernetes.io/component=server
                  app.kubernetes.io/instance=clamav
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=clamav
                  app.kubernetes.io/version=1.0.0
                  helm.sh/chart=clamav-1.7.34
                  pod-template-hash=7f69848b68
Annotations:      kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory limit for container clamav
Status:           Pending
IP:               10.244.0.6
IPs:
  IP:           10.244.0.6
Controlled By:  ReplicaSet/clamav-7f69848b68
Containers:
  clamav:
    Container ID:    
    Image:           docker.io/mkodockx/docker-clamav:1.1.2-alpine
    Image ID:        
    Port:            3310/TCP (clamav)
    Host Port:       0/TCP (clamav)
    SeccompProfile:  RuntimeDefault
    State:           Waiting
      Reason:        CreateContainerConfigError
    Ready:           False
    Restart Count:   0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     50m
      memory:  128Mi
    Liveness:  exec [/bin/bash -c echo 'PING' | nc 127.0.0.1 3310 | grep -i PONG
] delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/bin/bash -c echo 'PING' | nc 127.0.0.1 3310 | grep -i PONG
] delay=0s timeout=1s period=10s #success=1 #failure=3
    Startup:  exec [/bin/bash -c echo 'PING' | nc 127.0.0.1 3310 | grep -i PONG
] delay=0s timeout=1s period=10s #success=1 #failure=30
    Environment:
      TZ:  Etc/UTC
    Mounts:
      /etc/clamav/clamd.conf from clamd-config (rw,path="clamd.conf")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cddm7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  clamd-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      clamav
    Optional:  false
  kube-api-access-cddm7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason  Age                           From     Message
  ----     ------  ----                          ----     -------
  Normal   Pulled  4s (x1482 over 5h20m)         kubelet  Container image "docker.io/mkodockx/docker-clamav:1.1.2-alpine" already present on machine
  Warning  Failed  <invalid> (x1484 over 5h20m)  kubelet  Error: container has runAsNonRoot and image has non-numeric user (clamav), cannot verify user is non-root (pod: "clamav-7f69848b68-6k7kz_homelab(5f19eb9e-d1c4-48ae-b5a0-17d6032e7ca5)", container: clamav)


Name:             clamav-scan-zfdz8
Namespace:        homelab
Priority:         0
Service Account:  default
Node:             <none>
Labels:           batch.kubernetes.io/controller-uid=73cc0903-4a45-4ba5-9763-632fbf2179b0
                  batch.kubernetes.io/job-name=clamav-scan
                  controller-uid=73cc0903-4a45-4ba5-9763-632fbf2179b0
                  job-name=clamav-scan
Annotations:      kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container clamav; cpu, memory limit for container clamav
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    Job/clamav-scan
Containers:
  clamav:
    Image:      xrowpublic/helm-clamav:1.7.34
    Port:       <none>
    Host Port:  <none>
    Command:
      clamscan
      -r
      /
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:        250m
      memory:     256Mi
    Environment:  <none>
    Mounts:
      /downloads/complete from downloads (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5w6jg (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  downloads:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  downloads
    ReadOnly:   false
  kube-api-access-5w6jg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                   From               Message
  ----     ------            ----                  ----               -------
  Warning  FailedScheduling  11m (x63 over 5h18m)  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. not found
  Warning  FailedScheduling  10m                   default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. not found
  Warning  FailedScheduling  3m7s (x2 over 5m11s)  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. not found


Name:             hello-world-5f864d8b88-shjbm
Namespace:        homelab
Priority:         0
Service Account:  default
Node:             homelab/192.168.2.100
Start Time:       Sun, 21 Dec 2025 17:51:43 +0100
Labels:           app=hello-world-anubis
                  pod-template-hash=5f864d8b88
Annotations:      kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container nginx; cpu, memory limit for container nginx
Status:           Running
IP:               10.244.0.28
IPs:
  IP:           10.244.0.28
Controlled By:  ReplicaSet/hello-world-5f864d8b88
Containers:
  anubis:
    Container ID:    containerd://01efd43d11dc66cc70e68877dc2bc3373954b9ff5f5cdf561b218cc5d41e8746
    Image:           ghcr.io/techarohq/anubis:v1.22.0
    Image ID:        ghcr.io/techarohq/anubis@sha256:90eb7eab65b28e63bf11f9dedf4b34d599bcaab9d84e8f3d537f027f9ac96c62
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Sun, 21 Dec 2025 17:51:45 +0100
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:     250m
      memory:  256Mi
    Environment:
      BIND:                     :8080
      TARGET:                   http://localhost:80
      POLICY_FNAME:             /etc/anubis/policy.yaml
      OG_EXPIRY_TIME:           5m
      OG_PASSTHROUGH:           false
      SECURE_COOKIE:            true
      COOKIE_DOMAIN:            sanderpeters.cloud
      REDIRECT_DOMAINS:         sanderpeters.cloud
      USE_REMOTE_ADDRESS:       true
      ED25519_PRIVATE_KEY_HEX:  <set to the key 'ED25519_PRIVATE_KEY_HEX' in secret 'anubis-key'>  Optional: false
      DIFFICULTY:               12
    Mounts:
      /etc/anubis from anubis-policy (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-trx2p (ro)
  nginx:
    Container ID:   containerd://cb166cc4d91f81ac1a2ec49d6d127302f7dd2553711dbb1744d87b8cce730e0a
    Image:          nginx:latest
    Image ID:       docker.io/library/nginx@sha256:fb01117203ff38c2f9af91db1a7409459182a37c87cced5cb442d1d8fcc66d19
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 21 Dec 2025 17:51:47 +0100
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:        250m
      memory:     256Mi
    Environment:  <none>
    Mounts:
      /usr/share/nginx/html from html-content (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-trx2p (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  html-content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      hello-world-content
    Optional:  false
  anubis-policy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      anubis-policy
    Optional:  false
  kube-api-access-trx2p:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             transmission-flood-597bdbbf96-6gxts
Namespace:        homelab
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app=transmission-flood-anubis
                  pod-template-hash=597bdbbf96
Annotations:      kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container flood; cpu, memory limit for container flood
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    ReplicaSet/transmission-flood-597bdbbf96
Containers:
  anubis:
    Image:           ghcr.io/techarohq/anubis:v1.22.0
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:     250m
      memory:  256Mi
    Environment:
      BIND:                     :8080
      TARGET:                   http://localhost:3000
      POLICY_FNAME:             /etc/anubis/policy.yaml
      OG_EXPIRY_TIME:           5m
      OG_PASSTHROUGH:           false
      SECURE_COOKIE:            true
      COOKIE_DOMAIN:            flood.sanderpeters.cloud
      REDIRECT_DOMAINS:         flood.sanderpeters.cloud
      USE_REMOTE_ADDRESS:       true
      ED25519_PRIVATE_KEY_HEX:  <set to the key 'ED25519_PRIVATE_KEY_HEX' in secret 'anubis-key'>  Optional: false
      DIFFICULTY:               12
    Mounts:
      /etc/anubis from anubis-policy (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r5lmh (ro)
  transmission:
    Image:       lscr.io/linuxserver/transmission:latest
    Ports:       9091/TCP (web), 51413/TCP (torrent-tcp), 51413/UDP (torrent-udp)
    Host Ports:  0/TCP (web), 0/TCP (torrent-tcp), 0/UDP (torrent-udp)
    Limits:
      cpu:     1
      memory:  512Mi
    Requests:
      cpu:     100m
      memory:  256Mi
    Environment:
      PUID:  1000
      PGID:  1000
      TZ:    Europe/Amsterdam
      USER:  <set to the key 'username' in secret 'transmission-auth'>  Optional: false
      PASS:  <set to the key 'password' in secret 'transmission-auth'>  Optional: false
    Mounts:
      /config from transmission-config (rw)
      /downloads from downloads (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r5lmh (ro)
  flood:
    Image:      jesec/flood:latest
    Port:       3000/TCP (flood-web)
    Host Port:  0/TCP (flood-web)
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     250m
      memory:  256Mi
    Environment:
      FLOOD_OPTION_HOST:          0.0.0.0
      FLOOD_OPTION_PORT:          3000
      FLOOD_OPTION_TRANSMISSION:  true
      FLOOD_OPTION_TRURL:         http://localhost:9091/transmission/rpc
      FLOOD_OPTION_TRUSER:        <set to the key 'username' in secret 'transmission-auth'>  Optional: false
      FLOOD_OPTION_TRPASS:        <set to the key 'password' in secret 'transmission-auth'>  Optional: false
    Mounts:
      /data from flood-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r5lmh (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  transmission-config:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  transmission-flood-transmission-config
    ReadOnly:   false
  flood-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  transmission-flood-flood-data
    ReadOnly:   false
  downloads:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  downloads
    ReadOnly:   false
  anubis-policy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      anubis-policy
    Optional:  false
  kube-api-access-r5lmh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  3m7s  default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. not found


Name:             transmission-flood-74df665758-7smpq
Namespace:        homelab
Priority:         0
Service Account:  default
Node:             homelab/192.168.2.100
Start Time:       Sun, 21 Dec 2025 19:00:43 +0100
Labels:           app=transmission-flood-anubis
                  pod-template-hash=74df665758
Annotations:      kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container flood; cpu, memory limit for container flood
Status:           Running
IP:               10.244.0.31
IPs:
  IP:           10.244.0.31
Controlled By:  ReplicaSet/transmission-flood-74df665758
Containers:
  anubis:
    Container ID:    containerd://d41b0f83193302422288f5ee221087228a4cf30360493c5b713667ba9d9c2e75
    Image:           ghcr.io/techarohq/anubis:v1.22.0
    Image ID:        ghcr.io/techarohq/anubis@sha256:90eb7eab65b28e63bf11f9dedf4b34d599bcaab9d84e8f3d537f027f9ac96c62
    Port:            8080/TCP
    Host Port:       0/TCP
    SeccompProfile:  RuntimeDefault
    State:           Running
      Started:       Sun, 21 Dec 2025 19:00:45 +0100
    Ready:           True
    Restart Count:   0
    Limits:
      cpu:     750m
      memory:  256Mi
    Requests:
      cpu:     250m
      memory:  256Mi
    Environment:
      BIND:                     :8080
      TARGET:                   http://localhost:3000
      POLICY_FNAME:             /etc/anubis/policy.yaml
      OG_EXPIRY_TIME:           5m
      OG_PASSTHROUGH:           false
      SECURE_COOKIE:            true
      COOKIE_DOMAIN:            flood.sanderpeters.cloud
      REDIRECT_DOMAINS:         flood.sanderpeters.cloud
      USE_REMOTE_ADDRESS:       true
      ED25519_PRIVATE_KEY_HEX:  <set to the key 'ED25519_PRIVATE_KEY_HEX' in secret 'anubis-key'>  Optional: false
      DIFFICULTY:               12
    Mounts:
      /etc/anubis from anubis-policy (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-84hdq (ro)
  transmission:
    Container ID:   containerd://5fdbba2081742fd7a0567e579ea68fe23e621473f3f771e082fd77c4cb278872
    Image:          lscr.io/linuxserver/transmission:latest
    Image ID:       lscr.io/linuxserver/transmission@sha256:9e5157459da3272d5dcbca2db84f3823dd9daa0f166b963c5d51899098b17035
    Ports:          9091/TCP (web), 51413/TCP (torrent-tcp), 51413/UDP (torrent-udp)
    Host Ports:     0/TCP (web), 0/TCP (torrent-tcp), 0/UDP (torrent-udp)
    State:          Running
      Started:      Sun, 21 Dec 2025 19:00:47 +0100
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  512Mi
    Requests:
      cpu:     100m
      memory:  256Mi
    Environment:
      PUID:  1000
      PGID:  1000
      TZ:    Europe/Amsterdam
      USER:  <set to the key 'username' in secret 'transmission-auth'>  Optional: false
      PASS:  <set to the key 'password' in secret 'transmission-auth'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-84hdq (ro)
  flood:
    Container ID:   containerd://0300e25b1bdccf17997aa0f546faa6a80602f151e3c89fa24ec087632e195030
    Image:          jesec/flood:latest
    Image ID:       docker.io/jesec/flood@sha256:4b92628572d0b4ff740ca2d1e0262cf8e368cd39aa9e693a6dc2f681d71aadd2
    Port:           3000/TCP (flood-web)
    Host Port:      0/TCP (flood-web)
    State:          Running
      Started:      Sun, 21 Dec 2025 19:00:52 +0100
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:     250m
      memory:  256Mi
    Environment:
      FLOOD_OPTION_HOST:          0.0.0.0
      FLOOD_OPTION_PORT:          3000
      FLOOD_OPTION_TRANSMISSION:  true
      FLOOD_OPTION_TRURL:         http://localhost:9091/transmission/rpc
      FLOOD_OPTION_TRUSER:        <set to the key 'username' in secret 'transmission-auth'>  Optional: false
      FLOOD_OPTION_TRPASS:        <set to the key 'password' in secret 'transmission-auth'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-84hdq (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  html-content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      transmission-flood-content
    Optional:  false
  anubis-policy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      anubis-policy
    Optional:  false
  kube-api-access-84hdq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m24s  default-scheduler  Successfully assigned homelab/transmission-flood-74df665758-7smpq to homelab
  Normal  Pulled     9m22s  kubelet            Container image "ghcr.io/techarohq/anubis:v1.22.0" already present on machine
  Normal  Created    9m22s  kubelet            Created container: anubis
  Normal  Started    9m22s  kubelet            Started container anubis
  Normal  Pulling    9m22s  kubelet            Pulling image "lscr.io/linuxserver/transmission:latest"
  Normal  Pulled     9m20s  kubelet            Successfully pulled image "lscr.io/linuxserver/transmission:latest" in 1.384s (1.384s including waiting). Image size: 31325756 bytes.
  Normal  Created    9m20s  kubelet            Created container: transmission
  Normal  Started    9m20s  kubelet            Started container transmission
  Normal  Pulling    9m20s  kubelet            Pulling image "jesec/flood:latest"
  Normal  Pulled     9m16s  kubelet            Successfully pulled image "jesec/flood:latest" in 4.176s (4.176s including waiting). Image size: 73352893 bytes.
  Normal  Created    9m15s  kubelet            Created container: flood
  Normal  Started    9m15s  kubelet            Started container flood
